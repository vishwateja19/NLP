{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Text\\\\MyFile2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f454369bd00a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# store its reference in the variable file1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# and \"MyFile2.txt\" in D:\\Text in file2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfile2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"D:\\Text\\MyFile2.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w+\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Text\\\\MyFile2.txt'"
     ]
    }
   ],
   "source": [
    "file1 = open(\"MyFile.txt\",\"a\") \n",
    "\n",
    "# store its reference in the variable file1 \n",
    "# and \"MyFile2.txt\" in D:\\Text in file2 \n",
    "file2 = open(r\"D:\\Text\\MyFile2.txt\",\"w+\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading a text file in python\n",
    "file1 = open('car.txt', 'r') # file handle\n",
    "lines = file1.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The new Accord was also high on all kinds of tech and features oozing comfort and luxury on the inside. But it came at a price, the latest Accord was priced around 37 lakhs ex-showroom Delhi which stacked it right up with the German cars at that price point. The strong selling point of the Accord in India had always been providing luxury and performance at a price point much less than the German counterparts. But since that factor was no longer there, the new Accord bombed in the market. Frankly, no was willing to pay such a premium for a Honda. Soon this Accord was also discontinued in India with extremely few examples on the road.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT IS A TRUTH UNIVERSALLY ACKNOWLEDGED, THAT A SINGLE MAN IN POSSESSION OF A GOOD FORTUNE, MUST BE IN WANT OF A WIFE.\n"
     ]
    }
   ],
   "source": [
    "text = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n",
    "text = text.upper()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the new accord was also high on all kinds of tech and features oozing comfort and luxury on the inside. but it came at a price, the latest accord was priced around 37 lakhs ex-showroom delhi which stacked it right up with the german cars at that price point. the strong selling point of the accord in india had always been providing luxury and performance at a price point much less than the german counterparts. but since that factor was no longer there, the new accord bombed in the market. frankly, no was willing to pay such a premium for a honda. soon this accord was also discontinued in india with extremely few examples on the road.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[4].lower()  # to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE NEW ACCORD WAS ALSO HIGH ON ALL KINDS OF TECH AND FEATURES OOZING COMFORT AND LUXURY ON THE INSIDE. BUT IT CAME AT A PRICE, THE LATEST ACCORD WAS PRICED AROUND 37 LAKHS EX-SHOWROOM DELHI WHICH STACKED IT RIGHT UP WITH THE GERMAN CARS AT THAT PRICE POINT. THE STRONG SELLING POINT OF THE ACCORD IN INDIA HAD ALWAYS BEEN PROVIDING LUXURY AND PERFORMANCE AT A PRICE POINT MUCH LESS THAN THE GERMAN COUNTERPARTS. BUT SINCE THAT FACTOR WAS NO LONGER THERE, THE NEW ACCORD BOMBED IN THE MARKET. FRANKLY, NO WAS WILLING TO PAY SUCH A PREMIUM FOR A HONDA. SOON THIS ACCORD WAS ALSO DISCONTINUED IN INDIA WITH EXTREMELY FEW EXAMPLES ON THE ROAD.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[4].upper() # to upper case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife\n"
     ]
    }
   ],
   "source": [
    "#list comprehension\n",
    "text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "print(text_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IT IS A TRUTH UNIVERSALLY ACKNOWLEDGED, THAT A SINGLE MAN IN POSSESSION OF A GOOD FORTUNE, MUST BE IN WANT OF A WIFE.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=[]\n",
    "for char in text:\n",
    "    if  not char in string.punctuation:\n",
    "        text2.append(char)\n",
    "# split vs join:\n",
    "text2=''.join(text2).lower()        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assignment  : try to  remove punctuation using regular expressions : re!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################tokenization##############################:\n",
    "#breaking down a raw string into constituent words/characters.\n",
    "#various forms of tokenization : word tokenize,char tokenize,sentence level tokenization.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'new',\n",
       " 'Accord',\n",
       " 'was',\n",
       " 'also',\n",
       " 'high',\n",
       " 'on',\n",
       " 'all',\n",
       " 'kinds',\n",
       " 'of',\n",
       " 'tech',\n",
       " 'and',\n",
       " 'features',\n",
       " 'oozing',\n",
       " 'comfort',\n",
       " 'and',\n",
       " 'luxury',\n",
       " 'on',\n",
       " 'the',\n",
       " 'inside.',\n",
       " 'But',\n",
       " 'it',\n",
       " 'came',\n",
       " 'at',\n",
       " 'a',\n",
       " 'price,',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'Accord',\n",
       " 'was',\n",
       " 'priced',\n",
       " 'around',\n",
       " '37',\n",
       " 'lakhs',\n",
       " 'ex-showroom',\n",
       " 'Delhi',\n",
       " 'which',\n",
       " 'stacked',\n",
       " 'it',\n",
       " 'right',\n",
       " 'up',\n",
       " 'with',\n",
       " 'the',\n",
       " 'German',\n",
       " 'cars',\n",
       " 'at',\n",
       " 'that',\n",
       " 'price',\n",
       " 'point.',\n",
       " 'The',\n",
       " 'strong',\n",
       " 'selling',\n",
       " 'point',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Accord',\n",
       " 'in',\n",
       " 'India',\n",
       " 'had',\n",
       " 'always',\n",
       " 'been',\n",
       " 'providing',\n",
       " 'luxury',\n",
       " 'and',\n",
       " 'performance',\n",
       " 'at',\n",
       " 'a',\n",
       " 'price',\n",
       " 'point',\n",
       " 'much',\n",
       " 'less',\n",
       " 'than',\n",
       " 'the',\n",
       " 'German',\n",
       " 'counterparts.',\n",
       " 'But',\n",
       " 'since',\n",
       " 'that',\n",
       " 'factor',\n",
       " 'was',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'there,',\n",
       " 'the',\n",
       " 'new',\n",
       " 'Accord',\n",
       " 'bombed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'market.',\n",
       " 'Frankly,',\n",
       " 'no',\n",
       " 'was',\n",
       " 'willing',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'such',\n",
       " 'a',\n",
       " 'premium',\n",
       " 'for',\n",
       " 'a',\n",
       " 'Honda.',\n",
       " 'Soon',\n",
       " 'this',\n",
       " 'Accord',\n",
       " 'was',\n",
       " 'also',\n",
       " 'discontinued',\n",
       " 'in',\n",
       " 'India',\n",
       " 'with',\n",
       " 'extremely',\n",
       " 'few',\n",
       " 'examples',\n",
       " 'on',\n",
       " 'the',\n",
       " 'road.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[4].split()  # tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'new', 'Accord', 'was', 'also', 'high', 'on', 'all', 'kinds', 'of', 'tech', 'and', 'features', 'oozing', 'comfort', 'and', 'luxury', 'on', 'the', 'inside', '.', 'But', 'it', 'came', 'at', 'a', 'price', ',', 'the', 'latest', 'Accord', 'was', 'priced', 'around', '37', 'lakhs', 'ex-showroom', 'Delhi', 'which', 'stacked', 'it', 'right', 'up', 'with', 'the', 'German', 'cars', 'at', 'that', 'price', 'point', '.', 'The', 'strong', 'selling', 'point', 'of', 'the', 'Accord', 'in', 'India', 'had', 'always', 'been', 'providing', 'luxury', 'and', 'performance', 'at', 'a', 'price', 'point', 'much', 'less', 'than', 'the', 'German', 'counterparts', '.', 'But', 'since', 'that', 'factor', 'was', 'no', 'longer', 'there', ',', 'the', 'new', 'Accord', 'bombed', 'in', 'the', 'market', '.', 'Frankly', ',', 'no', 'was', 'willing', 'to', 'pay', 'such', 'a', 'premium', 'for', 'a', 'Honda', '.', 'Soon', 'this', 'Accord', 'was', 'also', 'discontinued', 'in', 'India', 'with', 'extremely', 'few', 'examples', 'on', 'the', 'road', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "words = word_tokenize(lines[4])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The new Accord was also high on all kinds of tech and features oozing comfort and luxury on the inside. But it came at a price, the latest Accord was priced around 37 lakhs ex-showroom Delhi which stacked it right up with the German cars at that price point. The strong selling point of the Accord in India had always been providing luxury and performance at a price point much less than the German counterparts. But since that factor was no longer there, the new Accord bombed in the market. Frankly, no was willing to pay such a premium for a Honda. Soon this Accord was also discontinued in India with extremely few examples on the road.\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # important words : new accord high all kinds tech features oozing comfort inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## list comprehension:###\n",
    "list2=[1,2,3,4,5]\n",
    "\n",
    "[x*x for x in list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_proc_text=[x for x in lines[4].split() if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The new Accord also high kinds tech features oozing comfort luxury inside. But came price, latest Accord priced around 37 lakhs ex-showroom Delhi stacked right German cars price point. The strong selling point Accord India always providing luxury performance price point much less German counterparts. But since factor longer there, new Accord bombed market. Frankly, willing pay premium Honda. Soon Accord also discontinued India extremely examples road.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(pre_proc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['truth', 'universally', 'acknowledged', 'single', 'man', 'possession', 'good', 'fortune', 'must', 'want', 'wife']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                 ####################next session###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming :\n",
    "lemmatization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root of a word : base form of any given word : \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the areas of Natural Language Processing we come across situation where two or more words have a common root. For example, the three words - agreed, agreeing and agreeable have the same root word agree. A search involving any of these words should treat them as the same word which is the root word. So it becomes essential to link all the words into their root word. The NLTK library has methods to do this linking and give the output showing the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming  :  reducing  a word to its root form. ex: agreed, agreeing and agreeable : --> agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Stem: It\n",
      "Actual: originated  Stem: origin\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: idea  Stem: idea\n",
      "Actual: that  Stem: that\n",
      "Actual: there  Stem: there\n",
      "Actual: are  Stem: are\n",
      "Actual: readers  Stem: reader\n",
      "Actual: who  Stem: who\n",
      "Actual: prefer  Stem: prefer\n",
      "Actual: learning  Stem: learn\n",
      "Actual: new  Stem: new\n",
      "Actual: skills  Stem: skill\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: comforts  Stem: comfort\n",
      "Actual: of  Stem: of\n",
      "Actual: their  Stem: their\n",
      "Actual: drawing  Stem: draw\n",
      "Actual: rooms  Stem: room\n"
     ]
    }
   ],
   "source": [
    "#Next find the roots of the word\n",
    "stem_words=[]\n",
    "for word in nltk_tokens:\n",
    "    print( \"Actual: %s  Stem: %s\"  % (word,porter_stemmer.stem(word)))\n",
    "    stem_words.append(porter_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_stem = ' '.join(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It origin from the idea that there are reader who prefer learn new skill from the comfort of their draw room'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is similar to stemming but it brings context to the words.So it goes a steps further by linking words with similar meaning to one word. For example if a paragraph has words like cars, trains and automobile, then it will link all of them to automobile. In the below program we use the WordNet lexical database for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples of lemmatization\n",
    "#joy,happiness--> happy\n",
    "#sadness, sad,grief--> sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shruthi S\n",
      "[nltk_data]     D\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: joy  Lemma: joy\n",
      "Actual: happy  Lemma: happy\n",
      "Actual: sad  Lemma: sad\n",
      "Actual: grief  Lemma: grief\n",
      "Actual: It  Lemma: It\n",
      "Actual: originated  Lemma: originated\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: idea  Lemma: idea\n",
      "Actual: that  Lemma: that\n",
      "Actual: there  Lemma: there\n",
      "Actual: are  Lemma: are\n",
      "Actual: readers  Lemma: reader\n",
      "Actual: who  Lemma: who\n",
      "Actual: prefer  Lemma: prefer\n",
      "Actual: learning  Lemma: learning\n",
      "Actual: new  Lemma: new\n",
      "Actual: skills  Lemma: skill\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: comforts  Lemma: comfort\n",
      "Actual: of  Lemma: of\n",
      "Actual: their  Lemma: their\n",
      "Actual: drawing  Lemma: drawing\n",
      "Actual: rooms  Lemma: room\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_data = \" joy happy sad grief It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for word in nltk_tokens:\n",
    "       print (\"Actual: %s  Lemma: %s\"  % (word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization : converting tokenized,cleaned text into meaning numeric repreentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "# count vectorizer\n",
    "# tfidf vectorizer\n",
    "#bag of words model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot encoding\n",
    "\n",
    "Suppose we have a sentence as “Can I eat the Pizza”.\n",
    "looking at this, we can directly say that all the words are different from each other but how will the machine know?\n",
    "so when we try to apply one hot ending i.e converting the categories into numerical labels.\n",
    "Firstly, convert the text to lower and then sort the words in ascending form i.e A-Z.\n",
    "Now we’ll have “can, eat, i, pizza, the”.\n",
    "Give a numerical label as we can see can is at 0th position and eat is at 1 same way, assign the values like can:0, i:2, eat:1, the:4, pizza:3.\n",
    "Transform to binary vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can(0) i(2) eat(1) the(4) pizza(3) # converting to lower case\n",
    "can eat i pizza the # sorting in ascending order.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index(can)-->0\n",
    "index(eat)-->1\n",
    "index(i)-->2\n",
    "index(pizza)-->3\n",
    "index(the) -->4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1. 0. 0. 0. 0.] #can\n",
    " [0. 0. 1. 0. 0.] #i\n",
    " [0. 1. 0. 0. 0.] #eat\n",
    " [0. 0. 0. 0. 1.] #the\n",
    " [0. 0. 0. 1. 0.]] #pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual line ='can i eat the pizza'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can     eat      i       pizza      the \n",
    "0       1        2         3         4\n",
    "\n",
    "\n",
    "1       0         0         0         0   -> can\n",
    "0       0         1         0         0   --> i\n",
    "0       1         0         0         0   -->eat\n",
    "0       0         0         0         1  -->the\n",
    "0       0         0         1         0  --> pizza\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(5,5)  # one hot  encoding : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#define example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_str=' '.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cold cold warm cold hot hot warm cold warm hot'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=in_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "data=le.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 1, 1, 2, 0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when working with 1D array as input in sklearn-->  convert into  2D\n",
    "data=data.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=OneHotEncoder()\n",
    "vect_data=ohe.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vect_data)  # csr ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cold cold warm cold hot hot warm cold warm hot'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_str  # --> matrix of shape(10,3)--> total 10 words and 3 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_data.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#29th oct-- > count vectorizer , tfidf vectorizer, naive bayes algorithm implementation\n",
    "#30th oct--> support vector machines(SVM)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CountVectorizer is a great tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "document = [\"One Geek helps Two Geeks\", \n",
    "\t\t\t\"Two Geeks help Four Geeks\", \n",
    "\t\t\t\"Each Geek helps many other Geeks at GeeksforGeeks\"] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[x.split() for x in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['One', 'Geek', 'helps', 'Two', 'Geeks'],\n",
       " ['Two', 'Geeks', 'help', 'Four', 'Geeks'],\n",
       " ['Each', 'Geek', 'helps', 'many', 'other', 'Geeks', 'at', 'GeeksforGeeks']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=[]\n",
    "for li in temp:\n",
    "    for it in li:\n",
    "        xx.append(it)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'one': 9, 'geek': 3, 'helps': 7, 'two': 11, 'geeks': 4, 'help': 6, 'four': 2, 'each': 1, 'many': 8, 'other': 10, 'at': 0, 'geeksforgeeks': 5}\n",
      "Encoded Document is:\n",
      "[[0 0 0 1 1 0 0 1 0 1 0 1]\n",
      " [0 0 1 0 2 0 1 0 0 0 0 1]\n",
      " [1 1 0 1 1 1 0 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "document = [\"One Geek helps Two Geeks\", \n",
    "\t\t\t\"Two Geeks help Four Geeks\", \n",
    "\t\t\t\"Each Geek helps many other Geeks at GeeksforGeeks\"] \n",
    "\n",
    "# Create a Vectorizer Object \n",
    "vectorizer = CountVectorizer() \n",
    "\n",
    "vector=vectorizer.fit_transform(document) \n",
    "\n",
    "# Printing the identified Unique words along with their indices \n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_) \n",
    "\n",
    "# Summarizing the Encoded Texts \n",
    "print(\"Encoded Document is:\") \n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=lines[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw back of count vect  and one hot : common words(and ,the ,on) are given equal importance as rare words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF vectorizer : term frequency - inverse document frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=corpus.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' But it came at a price, the latest Accord was priced around 37 lakhs ex-showroom Delhi which stacked it right up with the German cars at that price point'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at  : tf + idf  = 3 +\n",
    "accord  : tf + idf  =1 +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf('term')  :  term frequency : freq of the 'term' in that document.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df('term')  :  the number of documents in the corpus that contain the 'term'\n",
    "idf('term') : number of documents in the corpus /df('term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus -- >100 documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document 1 : the word 'the' appears-- >50 times and 'golden'  appears once. len(document 1)-->150\n",
    "tf('the')--> 50\n",
    "tf('golden')--> 1\n",
    "normalized tf('the')-- > 50/150\n",
    "normalized tf('golden') -- > 1/150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df('the')=75\n",
    "df('golden')=5\n",
    "idf('the') = 100/75=1  log(1.33  = 0.41\n",
    "idf('golden')  =100/5 =20  log(20) = 4.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf-idf('term') = tf('term')* idf('term')  # final formula for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf-idf('the')=  50*0.41 = 20.5  \n",
    "tf-idf('golden') = 5*4.32 = 24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('truth', 'NN'), ('universally', 'RB'), ('acknowledged', 'VBD'), ('single', 'JJ'), ('man', 'NN'), ('possession', 'NN'), ('good', 'JJ'), ('fortune', 'NN'), ('must', 'MD'), ('want', 'VB'), ('wife', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos = pos_tag(filtered_words)\n",
    "print(pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
